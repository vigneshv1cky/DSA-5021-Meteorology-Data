{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Australia rainfall probability verification data extraction\n",
    "\n",
    "24 March 2022\n",
    "\n",
    "See https://anaconda.org/bfiedler/australiaverificationdata \n",
    "\n",
    "This notebook extends that notebook to rainfall probability data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "#import matplotlib.pyplot as plt\n",
    "#from IPython.core.display import HTML\n",
    "#HTML( open('my_css.css').read() ) # if you don't have my_css.css, comment this line out\n",
    "from datetime import datetime as dt\n",
    "utc=dt.utcfromtimestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following adds some optional HTML color style to this jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> div.text_cell_render{background-color: #ded; font-size: 18px}\n",
       "                    .rendered_html code {background-color: #ded; color: #900; font-size: 16px}\n",
       "                    .rendered_html pre {background-color: #ded; color: #900; font-size: 16px} \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style> div.text_cell_render{background-color: #ded; font-size: 18px}\n",
    "                    .rendered_html code {background-color: #ded; color: #900; font-size: 16px}\n",
    "                    .rendered_html pre {background-color: #ded; color: #900; font-size: 16px} \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookforf=\"DailyPoP1\"\n",
    "lookforc='Mean number of days of rain >= 1 mm'\n",
    "# or this:\n",
    "#lookforf=\"DailyPoP10\"\n",
    "#lookforc='Mean number of days of rain >= 10 mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one\n",
    "#site='09021' #Perth\n",
    "site='31011' # Cairns\n",
    "#site='12038' #Kalgoorlie\n",
    "#---------------------\n",
    "# sites below have data issues, and should be avoided\n",
    "#site='97072' # Strahan\n",
    "#site='94008' # Hobart\n",
    "#site='86282' # Melbourne\n",
    "#site='15590' # Alice Springs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outfileext='.csv' # normally use this\n",
    "outfileext='.csvx'  # for testing, so you don't overwrite good file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdir = 'australia_study/' # the path to where you stored the grep data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "australia_study/31011obs.csv\n",
      "australia_study/31011fcst.csv\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(fcstfile)\n\u001b[1;32m      5\u001b[0m climofilex\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIDCJCM*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39msite\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m----> 6\u001b[0m climofile\u001b[38;5;241m=\u001b[39m\u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mclimofilex\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(climofile)\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "obfile=pdir+site+'obs.csv'\n",
    "print(obfile)\n",
    "fcstfile=pdir+site+'fcst.csv'\n",
    "print(fcstfile)\n",
    "climofilex='IDCJCM*'+site+'.csv' \n",
    "climofile=glob.glob(pdir+climofilex).pop()\n",
    "print(climofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=open(obfile).readlines()\n",
    "fcsts=open(fcstfile).readlines()\n",
    "climos=open(climofile).readlines()\n",
    "print('obs',len(obs))\n",
    "print('fcsts',len(fcsts))\n",
    "print('climos',len(climos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nline=0\n",
    "found=False\n",
    "\n",
    "for line in climos:\n",
    "    if not found: print(line,end='')\n",
    "    if nline==3: sitename=line.strip().replace('\"','')\n",
    "    nline+=1\n",
    "    if line.startswith('\"'+lookforc):\n",
    "        print(\"\\ngetting climate for \" +lookforc+\" from line:\")\n",
    "        print(line)\n",
    "        q=line.split(',')\n",
    "        tmxc=[float(x) for x in q[1:13] ]\n",
    "        print(\"-\"*80)\n",
    "        print(\"found the monthly \"+lookforc,\" for \"+sitename+\":\")\n",
    "        print(tmxc)\n",
    "        found=True\n",
    "if not found: print(\"\\nclimate for\"+lookforc+\" NOT found\")\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daysper=[31,28.25,31,30,31,30,31,31,30,31,30,31]\n",
    "popc=[round((100*x)/y) for x,y in zip(tmxc,daysper)]\n",
    "popc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An observation file\n",
    "(tip for jupyter notebooks: Shift-O toggles scrolling of long output to cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 60 lines of obs:\n",
    "print(*obs[0:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```grep``` process we used for constructing ```obs``` stripped the header lines of the csv files.  So here is the header of an original observation csv file:\n",
    "```\n",
    "station_number,area_code,parameter,valid_start,valid_end,value,unit,statistic,instantaneous,level,qc_valid_minutes,qc_valid_minutes_start,qc_valid_minutes_end\n",
    "```\n",
    "All time are in [epoch time](https://www.epoch101.com/), in units of seconds. Our python function ```utc```, defined above, does the conversion. Here are some examples of using ```utc```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtime = utc(1430402400)\n",
    "testtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1430431200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for line in obs[0:60]:\n",
    "    q=line.strip().split(',')\n",
    "    valid_start=int(q[3])\n",
    "    valid_end=int(q[4])\n",
    "    delta=valid_end-valid_start\n",
    "    print(q[2],\"valid_start:\", valid_start, utc(valid_start),  \"   valid_end:\", valid_end, utc(valid_end),delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the forecasted variable ```AIR_TEMP``` is for an instant in time, so ```valid_start=valid_end```.\n",
    "For other variables, such as ```AIR_TEMP_MAX```, ```valid_end-valid_start``` is 24 hour period.  The ```qc``` variables are also times, but generally are not of concern in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"observations for\",sitename)\n",
    "parl=''\n",
    "pars={}\n",
    "for line in obs:\n",
    "    q=line.split(',')\n",
    "    par=q[2]\n",
    "    unit=q[6]\n",
    "    statistic=q[7]\n",
    "    instantaneous=q[8]\n",
    "    if par!=parl:\n",
    "        if par not in pars:\n",
    "            pars[par]=[0,unit,statistic,instantaneous]\n",
    "        parl=par\n",
    "    pars[par][0]+=1\n",
    "for par in pars:\n",
    "    print(par,pars[par])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookfor='PRCP'\n",
    "obsx={}\n",
    "n=0\n",
    "j=0\n",
    "for line in obs:\n",
    "    q=line.strip().split(',')\n",
    "    if q[2]!=lookfor:\n",
    "        continue\n",
    "    valid_start=int(q[3])\n",
    "    if valid_start not in obsx:\n",
    "        obsx[valid_start]=line\n",
    "        n+=1\n",
    "    else:\n",
    "        j+=1\n",
    "        if line==obsx[valid_start]:\n",
    "            print(utc(valid_start),valid_start,\"duplicate\")\n",
    "        else:\n",
    "            print(utc(valid_start),valid_start,\"different\")\n",
    "            print(line,end='')\n",
    "            print(obsx[valid_start])\n",
    "print(sitename,\":  \",n,\" distinct entries for\",lookfor)\n",
    "print(j,\"problems with duplicate or different entries for same valid_start time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we check for missing data.  Ideally there would be a measurement given \n",
    "# every 3600 secs = 1 hr\n",
    "vs=sorted(obsx.keys())\n",
    "vl=vs[0]\n",
    "for v in vs[1:]:\n",
    "    timegap=v-vl\n",
    "    if timegap != 3600:\n",
    "        print(\"timegap>3600:  \",v, utc(v),\"timegap=\",timegap)\n",
    "    vl=v\n",
    "print(sitename,lookfor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A forecast file\n",
    "The weather forecasts are produced from a [numerical weather predicition](https://en.wikipedia.org/wiki/Numerical_weather_prediction) model (check out the nifty picture of the grid). The forecast model begins from what came out of the previous days results, but with nudging variables, [data assimilation](https://en.wikipedia.org/wiki/Data_assimilation), to conform with the various atmospheric observations that have been recently received. The updated model variables will not exactly conform with the previous pure forecasts for that time, or with the data received. There may be some interest to somebody in what the model was doing in the assimilation period in the past few days, but not us. For most of us, the model results for the future are what we need. Finding some assimilation output in our forecast file can be confusing. See below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First 4 lines of fcst:\n",
    "print(*fcsts[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```grep``` process we used for ```fcst``` stripped the first line, so here it is:\n",
    "```\n",
    "station_number,area_code,parameter,valid_start,valid_end,value,unit,statistic,instantaneous,level,base_time\n",
    "```\n",
    "We see that the first 4 lines are giving a forecast for Probability of Precipitation, or PoP.\n",
    "Let's examine the times in the first 4 lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sitename,fcstfile)\n",
    "print(\"valid_start                        valid_end                          base_time\",end=\"\")\n",
    "print(\"                 forecast hour\")\n",
    "for line in fcsts[0:4]:\n",
    "    q=line.strip().split(',')\n",
    "    valid_start=int(q[3])\n",
    "    valid_end=int(q[4])\n",
    "    base_time=int(q[-1])\n",
    "    forecasth = (valid_start-base_time)//3600\n",
    "    print(valid_start,utc(valid_start),\"   \",\n",
    "          valid_end, utc(valid_end),\"   \",base_time,utc(base_time),\"  \",forecasth )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sitename)\n",
    "pars={}\n",
    "for line in fcsts:\n",
    "    q=line.split(',')\n",
    "    par=q[2]\n",
    "    unit=q[6]\n",
    "    statistic=q[7]\n",
    "    instantaneous=q[8]\n",
    "    if par not in pars:\n",
    "        pars[par]=[0,unit,statistic,instantaneous]\n",
    "    pars[par][0]+=1\n",
    "for par in pars:\n",
    "    print(par,pars[par])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```base_time``` is the model time at which no more data will be added, and the model will be run into the future.  Al the ```base_time``` are apparently 0 UTC, or 10 am AEST.  A few hours [wall-clock time](https://en.wikipedia.org/wiki/Elapsed_real_time) will be needed for the computer program to finish making forecasts for future days.  The model forecast results would be released to the public a few hours after the base-time, which would be afternoon in Australia.  Here is the\n",
    "[Time zone information for Melbourne](https://www.timeanddate.com/time/zone/australia/melbourne)\n",
    "  Note that there are lines of data with ```valid_start<base_time```.  As explained above, we want to ignore those lines, and analyze genuine forecasts for the future, with the forecast hour positive.  \n",
    "\n",
    "Note that ```valid_end-valid_start``` is a 24 hour period for ```DailyPoP```, but it doesn't conform to the boundaries of midnight in Australia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc(1430924400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc(1431010800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find all forecasts for DailyPoP1 or other\n",
    "Let's find such forecasts in the file, and put them in a new list ```fcstx```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc(1430438400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lookhour=15 #22,20,10 # 15 for PoP\n",
    "fcstx=[]\n",
    "vss={}\n",
    "vsp={}\n",
    "for line in fcsts:\n",
    "    q=line.split(',')\n",
    "    valid_start=int(q[3])\n",
    "    valid_end=int(q[4])\n",
    "    if q[2]==lookforf:\n",
    "        fcstx.append(line)\n",
    "        hour=utc(valid_start).hour\n",
    "        spanh=(valid_end-valid_start)//3600\n",
    "        vss.setdefault(hour,0)\n",
    "        vsp.setdefault(spanh,0)\n",
    "        vss[hour]+=1\n",
    "        vsp[spanh]+=1\n",
    "print(sitename)\n",
    "print(\"number of line in fcst:\",len(fcsts),\"   number of lines with \"+lookforf,'hour=',lookhour, len(fcstx))\n",
    "print(\"start hours\",vss)\n",
    "print(\"spans\",vsp)\n",
    "if len(vss)!=1 or len(vsp)!=1:\n",
    "    print(sitename+\" may have Daylight savings time issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spanx=24 #  for PoP\n",
    "starth=15 # Cairns\n",
    "\n",
    "needset=set([starth+x*24 for x in range(7)])\n",
    "print(sitename)\n",
    "vss={}\n",
    "for line in fcstx:\n",
    "    q=line.strip().split(\",\")\n",
    "    base_time=int(q[-1])\n",
    "    valid_start=int(q[3])\n",
    "    valid_end=int(q[4])\n",
    "    if valid_start not in vss:\n",
    "        vss[valid_start]=[]\n",
    "    vss[valid_start].append([valid_end,base_time])\n",
    "kvs=sorted(vss.keys())\n",
    "kvl=0\n",
    "for kv in kvs:\n",
    "    eba=vss[kv]   \n",
    "    fha=[(kv-bt)//3600 for ve,bt in eba]\n",
    "    fha.reverse()\n",
    "    delta=kv-kvl\n",
    "    print(kv,delta,len(eba),fha,end='')\n",
    "    for ve,bt in eba:\n",
    "        span=(ve-kv)/3600\n",
    "        if span!= spanx: print(\"span problem:\",kv,ve)\n",
    "    if kv-3600 in kvs:\n",
    "        print(\" dst problem\",end='')\n",
    "    if starth not in fha:\n",
    "        print(\" gap hours\",end='')\n",
    "    if not needset <= set(fha):\n",
    "        print(\", missing forecast\",end='')\n",
    "    print()\n",
    "    kvl=kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsts=[]\n",
    "for line in fcstx:\n",
    "    q=line.strip().split(\",\")\n",
    "    base_time=int(q[-1])\n",
    "    if base_time not in bsts:\n",
    "        bsts.append(base_time)\n",
    "bsts.sort()\n",
    "btl=bsts[0]\n",
    "for bt in bsts[1:]:\n",
    "    timejump=bt-btl\n",
    "    if timejump>86400:\n",
    "        print(\"timejump=\",timejump,\"seconds,   missing forecasts from\",utc(btl+86400))\n",
    "    btl=bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookforf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdict={}\n",
    "needvr=24 # 15\n",
    "for line in fcsts:\n",
    "    q=line.strip().split(\",\")\n",
    "    if q[2]!=lookforf: continue\n",
    "    base_time=int(q[-1])\n",
    "    bt=utc(base_time)\n",
    "    if bt.hour !=0 : print(\"bt=\",bt.hour) \n",
    "    valid_start=int(q[3])\n",
    "    valid_end=int(q[4])\n",
    "    vrange = (valid_end-valid_start)//3600 \n",
    "    if vrange !=needvr: print(vrange)  \n",
    "    fhour=(valid_start - base_time)//3600\n",
    "    par=float(q[5])\n",
    "    if valid_start not in fdict:\n",
    "        fdict[valid_start]={}\n",
    "    if fhour not in fdict[valid_start]:\n",
    "        fdict[valid_start][fhour]=par\n",
    "    else:\n",
    "        print(\"duplicate\")\n",
    "print(len(fdict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kvs = sorted(fdict.keys())\n",
    "for kv in kvs:\n",
    "    fhs = sorted(fdict[kv].keys())\n",
    "    print(kv,utc(kv),fhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testkey=kvs[20] # 20 is arbitrary\n",
    "print(testkey,utc(testkey))\n",
    "fdict[testkey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsx[testkey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "odict={}\n",
    "spanx=24\n",
    "for kv in kvs:\n",
    "    hours = [kv+n*3600 for n in range(spanx)]\n",
    "    m = 0\n",
    "    Tsum=0.\n",
    "    for h in hours:\n",
    "        if h not in obsx: \n",
    "            #print(kv,h,(h-kv)//3600)\n",
    "            print(\"missing:\",h,\"for\",kv)\n",
    "            continue\n",
    "        else:\n",
    "            q = obsx[h].strip().split(',')\n",
    "            val = float(q[5])\n",
    "            if q[2]!=lookfor: print(q[2])\n",
    "            Tsum += val\n",
    "            m += 1\n",
    "    if m!=spanx:\n",
    "        print(\"nope\",kv,m)\n",
    "    else:\n",
    "        odict[kv] = Tsum\n",
    "print(len(odict),\"found days with \",spanx,\" hours \"+lookfor,\"for \",len(kvs),\"attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdict[testkey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edict={} # for export into cvs file\n",
    "n=0\n",
    "kvs=sorted(odict)\n",
    "kvw=kvs[0]\n",
    "for kv in kvs:\n",
    "    if kv!=kvw:\n",
    "        edict[kvw]={'o':'NA'}\n",
    "    edict[kv]={'o':odict[kv]}\n",
    "    kvw=kv+86400\n",
    "print(len(edict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(edict):\n",
    "    print(k, edict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdict[testkey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kv in sorted(edict):\n",
    "    month=utc(kv).month\n",
    "    edict[kv]['c']=popc[month-1]\n",
    "    for fn in [1,2,3,4,5,6,7]:\n",
    "        fh=hour+24*(fn-1)\n",
    "        if fh in fdict[kv]:\n",
    "            edict[kv][fn]=fdict[kv][fh]\n",
    "        else:\n",
    "            edict[kv][fn]='NA'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in sorted(edict):\n",
    "    print(k, edict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(x):\n",
    "    f=\"{: 6.1f} \"\n",
    "    if type(x)==type('a'):\n",
    "        return '   '+x+'  '\n",
    "    else:\n",
    "        return f.format(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zi(x):\n",
    "    f=\"{:5d} \"\n",
    "    if type(x)==type('a'):\n",
    "        return '   '+x+' '\n",
    "    else:\n",
    "        return f.format(int(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "header='   utc         ob   clim   f1    f2    f3    f4    f5    f6    f7'\n",
    "#print(header)\n",
    "outfilename=site+'_'+lookforf+outfileext\n",
    "outfile=open(outfilename,'w')\n",
    "outfile.write(header+'\\n')\n",
    "for kv in sorted(edict):\n",
    "    d=edict[kv]\n",
    "    os=repr(kv)+' '\n",
    "    os+=z(d['o'])\n",
    "    os+=zi(d['c'])\n",
    "    for fn in [1,2,3,4,5,6,7]:\n",
    "        os+=zi(d[fn])     \n",
    "#    print(os)\n",
    "    outfile.write(os+\"\\n\")\n",
    "outfile.close()                  \n",
    "print(\"written:\",outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outfilename) as testit:\n",
    "    for line in testit.readlines():\n",
    "        print(line, end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
