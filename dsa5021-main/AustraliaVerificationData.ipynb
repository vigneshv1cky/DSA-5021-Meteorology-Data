{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Australian Data for Forecast Verification \n",
    "\n",
    "16 August 2023\n",
    "\n",
    "see https://anaconda.org/bfiedler/australiarainfallverificationdata for extension to rainfall data\n",
    "\n",
    "Let's consider a weather forecast of the sort you are familiar with: for example, the maximum air temperature at a location two days hence.  Was the forecast good or bad? After we have data for the future day can *verify* the forecast.  We can invesitgate if the forecast is more accurate than a [persistance forecast](https://glossary.ametsoc.org/wiki/Persistence_forecast), which is simply forecasting that the maximum temperature two days hence will be the same as today's maximum temperature. We can investigate if the forecast is more accurate than a [climatological forecast](https://glossary.ametsoc.org/wiki/Climatological_forecast), which could be simply forecasting that the maximum temperature would be the average maximum temperature for the particular month, over many previous years.\n",
    "\n",
    "## Australian data\n",
    "For some reason, Australia has a tradition of offering datasets for forecast verification projects. Here we find 366 day data set of forecasts+observations at hundreds of automated weather stations: [Weather forecasting verification data (2015-05 to 2016-04) ](https://data.gov.au/data/dataset/weather-forecasting-verification-data-2015-05-to-2016-04/resource/16083945-3309-4c8a-9b64-49971be15878)\n",
    "But you don't need do download the 605 Mb zip file. **But you don't need to download any of this data.** I will extract for you a relevant subset of the data. Although interesting, you don't necessarily need the 17 page Verification Technical Reference. I hope to tell you what you need to know.\n",
    "\n",
    "### climate data\n",
    "[Climate statistics for Australian sites ](http://www.bom.gov.au/climate/averages/tables/ca_site_file_names.shtml)offers  monthly averages of the weather observations for many decades. For example,\n",
    "I click on *Tasmania*, then find *Strahan Aerodrome*, then clock on *monthly statistics*, which brings this page for site number [097072](http://www.bom.gov.au/climate/averages/tables/cw_097072_All.shtml).  If you look carefully you can see a link *Data file of statistics for this site (csv)* .  The clicking on that will deliver ```IDCJCM0033_097072.csv```\n",
    "\n",
    "*Care needs to be taken if analysing data against Local Time, as for part of the year many Australian \n",
    "states adopt Daylight Saving Time (DST), and observers continue to take observations according to the \n",
    "local clock.  This means that a 9am observation taken in a period of Daylight Saving is really an 8am \n",
    "observation in Local Standard Time.  For elements such as temperature the difference can be \n",
    "significant, as the temperature at this time of day is often rising quickly.*\n",
    "\n",
    "[notes about Aussie climate files](http://www.bom.gov.au/climate/how/newproducts/images/IDCJHC02_notes.txt)\n",
    "\n",
    "  \n",
    "[Australian Climate](http://www.bom.gov.au/climate/) provides an overview, with maps, of Australia climate.\n",
    " [Australian Climate Extremes](https://www.ga.gov.au/scientific-topics/national-location-information/dimensions/climatic-extremes) provides some ideas of sites that could provide some weather forecast challenges.\n",
    " \n",
    "### A subset of forecast/observation data. \n",
    "If you were to unzip the 605 MB file, you would get a forecast and observation file for each of 366 days, with all sites conglomerated into one file.  But we want to focus on one site. Here is an example of what I have done for you from the linux command line:\n",
    "```\n",
    "cd BoM_ETA_20150501-20160430\n",
    "grep -h ^97072, obs/*  > 97072obs.csv\n",
    "grep -h ^97072, fcst/* > 97072fcst.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "#import matplotlib.pyplot as plt\n",
    "#from IPython.core.display import HTML\n",
    "#HTML( open('my_css.css').read() ) # if you don't have my_css.css, comment this line out\n",
    "from datetime import datetime as dt\n",
    "utc=dt.utcfromtimestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following adds some optional HTML color style to this jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style> div.text_cell_render{background-color: #ded; font-size: 18px}\n",
    "                    .rendered_html code {background-color: #ded; color: #900;font-size: 16px}\n",
    "                    .rendered_html pre {background-color: #ded; color: #900; font-size: 16px} \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I avoid all sites that use Daylight Savings Time. For example a 4-day forecast and a 3-day forecast for the maximum temperature during a 15 hour period of a certain day WILL NOT be the same 15 hour span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one\n",
    "#site='09021' #Perth\n",
    "#site='31011' # Cairns\n",
    "site='12038' #Kalgoorlie\n",
    "#---------------------\n",
    "# sites below have data issues, DSY and otherwise,  and should be avoided\n",
    "#site='97072' # Strahan\n",
    "#site='94008' # Hobart\n",
    "#site='86282' # Melbourne\n",
    "#site='15590' # Alice Springs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookfor='AIR_TEMP_MAX'\n",
    "lookforc='Mean maximum temperature'\n",
    "lookforf=\"MaxT\"\n",
    "\n",
    "## or these three:\n",
    "\n",
    "#lookfor='AIR_TEMP_MIN'\n",
    "#lookforc='Mean minimum temperature'\n",
    "#lookforf=\"MinT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outfileext='.csv' # normally use this\n",
    "outfileext='.csvx'  # for testing, so you don't overwrite good file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdir = 'australia_study/' # the path to where you store grep data  files, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obfile=pdir+site+'obs.csv'\n",
    "print(obfile)\n",
    "fcstfile=pdir+site+'fcst.csv'\n",
    "print(fcstfile)\n",
    "climofilex='IDCJCM*'+site+'.csv' \n",
    "climofile=glob.glob(pdir+climofilex).pop()\n",
    "print(climofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=open(obfile).readlines()\n",
    "fcsts=open(fcstfile).readlines()\n",
    "climos=open(climofile).readlines()\n",
    "print('obs',len(obs))\n",
    "print('fcsts',len(fcsts))\n",
    "print('climos',len(climos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nline=0\n",
    "found=False\n",
    "\n",
    "for line in climos:\n",
    "    if not found: print(line,end='')\n",
    "    if nline==3: sitename=line.strip().replace('\"','')\n",
    "    nline+=1\n",
    "    if line.startswith('\"'+lookforc):\n",
    "        print(\"\\ngetting climate for \" +lookforc+\" from line:\")\n",
    "        print(line)\n",
    "        q=line.split(',')\n",
    "        tmxc=[float(x) for x in q[1:13] ]\n",
    "        print(\"-\"*80)\n",
    "        print(\"found the monthly \"+lookforc,\" tmxc for \"+sitename+\":\")\n",
    "        print(tmxc)\n",
    "        found=True\n",
    "if not found: print(\"\\nclimate for\"+lookforc+\" NOT found\")\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An observation file\n",
    "(tip for jupyter notebooks: Shift-O toggles scrolling of long output to cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First 60 lines of obs:\n",
    "print(*obs[0:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```grep``` process we used for constructing ```obs``` stripped the header lines of the csv files.  So here is the header of an original observation csv file:\n",
    "```\n",
    "station_number,area_code,parameter,valid_start,valid_end,value,unit,statistic,instantaneous,level,qc_valid_minutes,qc_valid_minutes_start,qc_valid_minutes_end\n",
    "```\n",
    "All time are in [epoch time](https://www.epoch101.com/), in units of seconds. Our python function ```utc```, defined above, does the conversion. Here are some examples of using ```utc```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtime = utc(1430402400)\n",
    "testtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1430431200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for line in obs[0:60]:\n",
    "    q=line.strip().split(',')\n",
    "    valid_start=int(q[3])\n",
    "    valid_end=int(q[4])\n",
    "    delta=valid_end-valid_start\n",
    "    print(q[2],\"valid_start:\", valid_start, utc(valid_start),  \"   valid_end:\", valid_end, utc(valid_end),delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the forecasted variable ```AIR_TEMP``` is for an instant in time, so ```valid_start=valid_end```.\n",
    "For other variables, such as ```AIR_TEMP_MAX```, ```valid_end-valid_start``` is 1 hour.  The ```qc``` variables are also times, but generally are not of concern in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"observations for\",sitename)\n",
    "parl=''\n",
    "pars={}\n",
    "for line in obs:\n",
    "    q=line.split(',')\n",
    "    par=q[2]\n",
    "    unit=q[6]\n",
    "    statistic=q[7]\n",
    "    instantaneous=q[8]\n",
    "    if par!=parl:\n",
    "        if par not in pars:\n",
    "            pars[par]=[0,unit,statistic,instantaneous]\n",
    "        parl=par\n",
    "    pars[par][0]+=1\n",
    "for par in pars:\n",
    "    print(par,pars[par])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Alice Springs data has multiple entries of the same observation.\n",
    "#You can see the AIR_TEMP is different and qc_valid_minutes_start is different by 1800 secs.\n",
    "\n",
    "obsx={}\n",
    "n=0\n",
    "j=0\n",
    "for line in obs:\n",
    "    q=line.strip().split(',')\n",
    "    if q[2]!=lookfor:\n",
    "        continue\n",
    "    valid_start=int(q[3])\n",
    "    if valid_start not in obsx:\n",
    "        obsx[valid_start]=line\n",
    "        n+=1\n",
    "    else:\n",
    "        j+=1\n",
    "        if line==obsx[valid_start]:\n",
    "            print(utc(valid_start),valid_start,\"duplicate\")\n",
    "        else:\n",
    "            print(utc(valid_start),valid_start,\"different\")\n",
    "            print(line,end='')\n",
    "            print(obsx[valid_start])\n",
    "print(sitename,\":  \",n,\" distinct entries for\",lookfor)\n",
    "print(j,\"problems with duplicate or different entries for same valid_start time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we check for missing data.  Ideally there would be a measurement given \n",
    "# every 3600 secs = 1 hr\n",
    "vs=sorted(obsx.keys())\n",
    "vl=vs[0]\n",
    "for v in vs[1:]:\n",
    "    timegap=v-vl\n",
    "    if timegap != 3600:\n",
    "        print(\"timegap>3600:  \",v, utc(v),\"timegap=\",timegap)\n",
    "    vl=v\n",
    "print(sitename,lookfor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A forecast file\n",
    "The weather forecasts are produced from a [numerical weather predicition](https://en.wikipedia.org/wiki/Numerical_weather_prediction) model (check out the nifty picture of the grid). The forecast model begins from what came out of the previous days results, but with nudging variables, [data assimilation](https://en.wikipedia.org/wiki/Data_assimilation), to conform with the various atmospheric observations that have been recently received. The updated model variables will not exactly conform with the previous pure forecasts for that time, or with the data received. There may be some interest to somebody in what the model was doing in the assimilation period in the past few days, but not us. For most of us, the model results for the future are what we need. Finding some assimilation output in our forecast file can be confusing. See below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First 4 lines of fcst:\n",
    "print(*fcsts[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```grep``` process we used for ```fcst``` stripped the first line, so here it is:\n",
    "```\n",
    "station_number,area_code,parameter,valid_start,valid_end,value,unit,statistic,instantaneous,level,base_time\n",
    "```\n",
    "We see that the first 4 lines are giving a forecast for Probability of Precipitation, or PoP.\n",
    "Let's examine the times in the first 4 lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sitename,fcstfile)\n",
    "print(\"valid_start                        valid_end                          base_time\",end=\"\")\n",
    "print(\"                 forecast hour\")\n",
    "for line in fcsts[0:4]:\n",
    "    q=line.strip().split(',')\n",
    "    valid_start = int(q[3])\n",
    "    valid_end = int(q[4])\n",
    "    base_time = int(q[-1])\n",
    "    forecasth = (valid_start-base_time)//3600\n",
    "    print(valid_start,utc(valid_start),\"   \",\n",
    "          valid_end, utc(valid_end),\"   \",base_time,utc(base_time),\"  \",forecasth )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sitename)\n",
    "pars={}\n",
    "for line in fcsts:\n",
    "    q=line.split(',')\n",
    "    par=q[2]\n",
    "    unit=q[6]\n",
    "    statistic=q[7]\n",
    "    instantaneous=q[8]\n",
    "    if par not in pars:\n",
    "        pars[par]=[0,unit,statistic,instantaneous]\n",
    "    pars[par][0]+=1 # number of lines with par\n",
    "for par in pars:\n",
    "    print(par,pars[par])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```base_time``` is the model time at which no more data will be added, and the model will be run into the future.  All the ```base_time``` are apparently 0 UTC, or 10 am AEST.  A few hours [wall-clock time](https://en.wikipedia.org/wiki/Elapsed_real_time) will be needed for the computer program to finish making forecasts for future days.  The model forecast results would be released to the public a few hours after the base-time, which would be afternoon in Australia.  Here is the\n",
    "[Time zone information for Melbourne](https://www.timeanddate.com/time/zone/australia/melbourne)\n",
    "  Note that there are lines of data with ```valid_start<base_time```.  As explained above, we want to ignore those lines, and analyze genuine forecasts for the future, with the forecast hour positive.  \n",
    "\n",
    "Note that ```valid_end-valid_start``` is a 24 hour period for ```DailyPoP```, but it doesn't conform to the boundaries of midnight in Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find all forecasts for MaxT or other\n",
    "\n",
    "```MaxT``` is the **forecast** for the maximum observed surface temperature in a 15 hour period.   Let's find such forecasts in the file, and put them in a new list ```fcstx```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fcstx=[]\n",
    "vss={} # valid starts\n",
    "vsp={} # valid spans\n",
    "for line in fcsts:\n",
    "    q=line.split(',')\n",
    "    valid_start=int(q[3])\n",
    "    valid_end=int(q[4])\n",
    "    if q[2]==lookforf:\n",
    "        fcstx.append(line)\n",
    "        hour=utc(valid_start).hour\n",
    "        spanh=(valid_end-valid_start)//3600\n",
    "        vss.setdefault(hour,0)\n",
    "        vsp.setdefault(spanh,0)\n",
    "        vss[hour]+=1\n",
    "        vsp[spanh]+=1\n",
    "print(sitename)\n",
    "print(\"number of line in fcst:\",len(fcsts),\"   number of lines with \"+lookforf,len(fcstx))\n",
    "print(\"start hours\",vss)\n",
    "print(\"spans\",vsp)\n",
    "if len(vss)!=1 or len(vsp)!=1:\n",
    "    print(sitename+\" may have Daylight savings time issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spanx=15 #  15 for MaxT, 15 for MinT\n",
    "\n",
    "if 'MIN' in lookfor:\n",
    "    if 'CAIRNS' in sitename:\n",
    "        starth = 8\n",
    "    else:\n",
    "        starth=10\n",
    "else:\n",
    "    if 'CAIRNS' in sitename:\n",
    "        starth = 20\n",
    "    else:\n",
    "        starth=22\n",
    "\n",
    "needset=set([starth+x*24 for x in range(7)])\n",
    "print(sitename)\n",
    "vss={}\n",
    "for line in fcstx:\n",
    "    q=line.strip().split(\",\")\n",
    "    base_time=int(q[-1])\n",
    "    valid_start=int(q[3])\n",
    "    valid_end=int(q[4])\n",
    "    if valid_start not in vss:\n",
    "        vss[valid_start]=[]\n",
    "    vss[valid_start].append([valid_end,base_time])\n",
    "kvs=sorted(vss.keys())\n",
    "kvl=0\n",
    "for kv in kvs:\n",
    "    eba=vss[kv]   \n",
    "    fha=[(kv-bt)//3600 for ve,bt in eba]\n",
    "    fha.reverse()\n",
    "    delta=kv-kvl\n",
    "    print(kv,delta,len(eba),fha,end='')\n",
    "    for ve,bt in eba:\n",
    "        span=(ve-kv)/3600\n",
    "        if span!= spanx: print(\"span problem:\",kv,ve)\n",
    "    if kv-3600 in kvs:\n",
    "        print(\" dst problem\",end='')\n",
    "    if starth not in fha:\n",
    "        print(\" gap hours\",end='')\n",
    "    if not needset <= set(fha):\n",
    "        print(\", missing forecast\" , end='')\n",
    "    #print(utc(base_time).hour)\n",
    "    print()\n",
    "    kvl=kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bsts=[]\n",
    "for line in fcstx:\n",
    "    q=line.strip().split(\",\")\n",
    "    base_time=int(q[-1])\n",
    "    if base_time not in bsts:\n",
    "        bsts.append(base_time)\n",
    "bsts.sort()\n",
    "btl=bsts[0]\n",
    "for bt in bsts[1:]:\n",
    "    timejump=bt-btl\n",
    "    if timejump>86400:\n",
    "        print(\"timejump=\",timejump,\"seconds,   missing forecasts from\",utc(btl+86400))\n",
    "    btl=bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookforf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdict={}\n",
    "needvr=15 # 15\n",
    "for line in fcsts:\n",
    "    q=line.strip().split(\",\")\n",
    "    if q[2]!=lookforf: continue\n",
    "    base_time=int(q[-1])\n",
    "    bt=utc(base_time)\n",
    "    if bt.hour !=0 : print(\"bt=\",bt.hour) \n",
    "    valid_start=int(q[3])\n",
    "    valid_end=int(q[4])\n",
    "    vrange = (valid_end-valid_start)//3600 \n",
    "    if vrange !=needvr: print(vrange)  \n",
    "    fhour=(valid_start - base_time)//3600\n",
    "    par=float(q[5])\n",
    "    if valid_start not in fdict:\n",
    "        fdict[valid_start]={}\n",
    "    if fhour not in fdict[valid_start]:\n",
    "        fdict[valid_start][fhour]=par\n",
    "    else:\n",
    "        print(\"duplicate\")\n",
    "print(len(fdict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kvs = sorted(fdict.keys())\n",
    "for kv in kvs:\n",
    "    fhs = sorted(fdict[kv].keys())\n",
    "    print(kv,utc(kv),fhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testkey=kvs[20] # 20 is arbitrary\n",
    "print(testkey,utc(testkey))\n",
    "fdict[testkey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsx[testkey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odict={}\n",
    "spanx=15\n",
    "for kv in kvs:\n",
    "    hours = [kv+n*3600 for n in range(spanx)]\n",
    "    m = 0\n",
    "    for h in hours:\n",
    "        if h not in obsx: \n",
    "            #print(kv,h,(h-kv)//3600)\n",
    "            print(\"missing:\",h,\"for\",kv)\n",
    "            continue\n",
    "        else:\n",
    "            q = obsx[h].strip().split(',')\n",
    "            val = float(q[5])\n",
    "            if q[2]!=lookfor: print(q[2])\n",
    "            if m==0:\n",
    "                Tmax=val\n",
    "            else:\n",
    "                if \"MAX\" in lookfor:\n",
    "                    Tmax=max(val,Tmax)\n",
    "                else:\n",
    "                    Tmax=min(val,Tmax)\n",
    "            m += 1\n",
    "    if m!=spanx:\n",
    "        print(\"span problem:\",kv,m,\" should be\",spanx)\n",
    "    else:\n",
    "#        Tavg = Tsum/m\n",
    "        odict[kv] = Tmax\n",
    "#        print(kv,Tavg)\n",
    "print(len(odict),\"found days with \",spanx,\" hours \"+lookfor,\"for \",len(kvs),\"attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc(testkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdict[testkey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odict[testkey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edict={} # combine obs and forecast, for export into cvs file\n",
    "n=0\n",
    "kvs=sorted(odict)\n",
    "kvw=kvs[0]\n",
    "for kv in kvs:\n",
    "    if kv!=kvw:\n",
    "        edict[kvw]={'o':'NA'}\n",
    "    edict[kv]={'o':odict[kv]}\n",
    "    kvw=kv+86400\n",
    "print(len(edict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kv in sorted(edict):\n",
    "    month=utc(kv).month\n",
    "    edict[kv]['c']=tmxc[month-1]\n",
    "    for fn in [1,2,3,4,5,6,7]:\n",
    "        fh=hour+24*(fn-1)\n",
    "        if fh in fdict[kv]:\n",
    "            edict[kv][fn]=fdict[kv][fh]\n",
    "        else:\n",
    "            edict[kv][fn]='NA'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(x):\n",
    "    f=\"{: .1f} \"\n",
    "    if type(x)==type('a'):\n",
    "        return '  '+x+'  '\n",
    "    else:\n",
    "        return f.format(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "header='   utc      ob    clim   f1    f2    f3    f4    f5    f6    f7'\n",
    "outfilename=site+'_'+lookfor+outfileext\n",
    "outfile=open(outfilename,'w')\n",
    "outfile.write(header+'\\n')\n",
    "for kv in sorted(edict):\n",
    "    d=edict[kv]\n",
    "    os=repr(kv)+' '\n",
    "    os+=z(d['o'])\n",
    "    os+=z(d['c'])\n",
    "    for fn in [1,2,3,4,5,6,7]:\n",
    "        os+=z(d[fn])     \n",
    "    outfile.write(os+\"\\n\")\n",
    "outfile.close()                  \n",
    "print(outfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File has been written.\n",
    "<hr style=\"height:30px;border:none;background-color:#f00;\" />\n",
    "Let's look at the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open(outfilename,'r'):\n",
    "    print(line,end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
